[
  {
    "objectID": "mini_project_penguin.html",
    "href": "mini_project_penguin.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "Our data comes from the events page in Penguin Random House. You can find the events page using [the Penguin Random House events page]: (https://www.penguinrandomhouse.com/authors/events/).\n\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(polite)\nlibrary(sf)\nlibrary(maps)\nlibrary(viridis)\nlibrary(leaflet) \nlibrary(htmltools)\nlibrary(httr) \nlibrary(httr2) \nlibrary(janitor)"
  },
  {
    "objectID": "mini_project_penguin.html#penguin-random-house-web-scraping",
    "href": "mini_project_penguin.html#penguin-random-house-web-scraping",
    "title": "Mini Project 2",
    "section": "",
    "text": "Our data comes from the events page in Penguin Random House. You can find the events page using [the Penguin Random House events page]: (https://www.penguinrandomhouse.com/authors/events/).\n\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(polite)\nlibrary(sf)\nlibrary(maps)\nlibrary(viridis)\nlibrary(leaflet) \nlibrary(htmltools)\nlibrary(httr) \nlibrary(httr2) \nlibrary(janitor)"
  },
  {
    "objectID": "mini_project_penguin.html#ethical-considerations",
    "href": "mini_project_penguin.html#ethical-considerations",
    "title": "Mini Project 2",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\nWe opted to use the robot.txt paths to determine if data from Penguin’s book events was permitted for web scraping. The site’s robots.txt file allowed our bots to access and scrape the data. While we considered implementing a polite function to ensure a respectful approach to data retrieval, this step appeared unnecessary given that the data is public and intended for widespread use."
  },
  {
    "objectID": "mini_project_penguin.html#novel-insights-potential-and-justification",
    "href": "mini_project_penguin.html#novel-insights-potential-and-justification",
    "title": "Mini Project 2",
    "section": "Novel Insights Potential and Justification",
    "text": "Novel Insights Potential and Justification\nOur final tibble will hold important information for booksellers, authors, agents, and students to utilize in regards to books/authors from Penguin Random House. We were initially motivated to explore book events from Penguin Random House to inform student decisions to network with agents and authors at various events.\nStudents can use our data to answer questions such as:\n\n“Where are events most commonly held?”\n“Which season has the most book events?”\n“What are the best events to attend to network with the right authors and book genres?”\n\nUpon further reflection, we discovered that our data could also be used by booksellers, book agents, and authors. Booksellers and authors may find our data useful because they can analyze current trends with where authors are going (chain or independent bookstore) and what authors are successful in book events (if we assume multiple book events equals a marketable author).Book agents within Penguin Random House or outside of it (smaller boutique literary agencies or other Big Five publishers) can use our data to answer questions on which authors are holding events, when a certain book is no longer welcomed in event spaces, and perhaps even publicity tactics. This data has relevant applications for different data needs within the publishing industry and for creating engaging data visualizations (including static or leaflet maps).\n\n#Step 0: Check if the website allows scraping \nrobotstxt::paths_allowed(\"https://www.penguinrandomhouse.com/authors/events/\")\n\n[1] TRUE\n\n#Extract individual information from the events page \ninfo_from_page &lt;- function(event, css_selector) {\n  read_html(event) |&gt; \n#Extracting nodes from the XML by using the CSS path from selector\n  html_nodes(css_selector) |&gt; \n#Extracting text\n  html_text()\n}\n\n#Test, the function works\ninfo_from_page(\"https://www.penguinrandomhouse.com/authors/events/\", \".date-display\")\n\n[1] \"November 2024\"\n\n\n\n#Scrape info using the CSS path and compile it into a tibble \nscrape_events &lt;- function(url){\n  \n  date &lt;- info_from_page(url, \".start\")\n  book &lt;- info_from_page(url, \".author-of a\")\n  author &lt;- info_from_page(url, \".author-name:nth-child(1)\")\n  host &lt;- info_from_page(url, \".event-location .hdr\")\n  state &lt;- info_from_page(url, \"span:nth-child(4)\")\n  zip_code &lt;- info_from_page(url, \"span:nth-child(5)\")\n  \n  tibble(date = date, \n           book = book, \n           author = author,\n           host = host,\n           state = state)\n  \n}\n\n\n#Test to see that our tibble looks appropriate\nscrape_events(\"https://www.penguinrandomhouse.com/authors/events/?page=2\")\n\n# A tibble: 10 × 5\n   date                   book                         author        host  state\n   &lt;chr&gt;                  &lt;chr&gt;                        &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;\n 1 11/28/2024 at 7:00     What She Said                Elizabeth Re… \"Bur… ON   \n 2 11/28/2024 at 7:00 pm  The Good Allies              Tim Cook      \"She… ON   \n 3 11/29/2024 at 10am     Rainier’s Legacy             Chad Corrie   \"The… MN   \n 4 11/29/2024             I Am the Grinch              Tom Brannon   \"Joi… AZ   \n 5 11/29/2024 at 5 PM     Alice in a Winter Wonderland Jan Brett     \"Oys… NH   \n 6 11/30/2024 at 12:00    Rediscovering Christmas      AJ Sherrill   \"Bar… SC   \n 7 11/30/2024 at 1:00 PM  Why We Love Football         Joe Posnanski \"BAR… NC   \n 8 11/30/2024 at 1pm      Murder at Glenloch Hill      Clara McKenna \"DRA… IA   \n 9 11/30/2024 at 10:30 AM Sari Sisters                 Anoosha Syed  \"Nei… TX   \n10 11/30/2024 at 10:00AM  Beast of the North Woods     Annelise Ryan \"Lit… WI   \n\n\n\n#This for loop runs all of the months and all of the days\n#   in one chunk but it is not the most efficient \n\n#If someone is interested in keeping this method in one chunk\n#   they can use this code for the nested for loop.\n\n#Nested for loop with i for months and j for days. \nfor(i in c(10, 11, 12, 1, 2, 3, 4)){\n#Runs to find data for all of the dates in these months\n#   we can compile all of the data together \n  for(j in 1:31){\n#Combining i and j for the dates to keep track of event dates\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    scrape_events(url)\n  }\n}\n\n\n#Test chunk to see if our previous code worked with a smaller set of data\n\n#If you wanted to run the previous code chunk, this test \n#   proves that it will give you a larger version\n\ni=11\nj=1\ndate = str_c(i, \"/\", j, \"/\", \"2024\")\nurl = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\nscrape_events(url)\n\n# A tibble: 0 × 5\n# ℹ 5 variables: date &lt;chr&gt;, book &lt;chr&gt;, author &lt;chr&gt;, host &lt;chr&gt;, state &lt;chr&gt;\n\n\n\n#Running each individual month as a separate \n#   for loop to be more efficient\n\n#Create a list to store your scraped data\noctober &lt;- list()\n  i=10\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    october[[j]] &lt;- scrape_events(url)\n}\n\n#Create a tibble from the list \noctober_tibble &lt;- bind_rows(october) |&gt; \n  as_tibble()\n  \nnovember &lt;- list()\n  i=11\nfor(j in 1:30){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    november[[j]] &lt;- scrape_events(url)\n}\n  \n  november_tibble &lt;- bind_rows(november) |&gt; \n    as_tibble() \n  \ndecember &lt;- list()\n  i=12\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    december[[j]] &lt;- scrape_events(url)\n}\n  \n  december_tibble &lt;- bind_rows(december) |&gt; \n    as_tibble()\n  \njanuary &lt;- list()\n  i=1\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    january[[j]] &lt;- scrape_events(url)\n}\n  \njanuary_tibble &lt;- bind_rows(january) |&gt; \n  as_tibble()\n\nfebruary &lt;- list()\n  i=2\nfor(j in 1:28){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    february[[j]] &lt;- scrape_events(url)\n}\n  \n  february_tibble &lt;- bind_rows(february) |&gt; \n    as_tibble() \n  \nmarch &lt;- list()\n  i=3\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    march[[j]] &lt;- scrape_events(url)\n}\n  \n  march_tibble &lt;- bind_rows(march) |&gt; \n    as_tibble() \n  \napril &lt;- list()\n  i=4\nfor(j in 1:30){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    april[[j]] &lt;- scrape_events(url)\n}\n  \napril_tibble &lt;- bind_rows(april) |&gt; \n  as_tibble()"
  },
  {
    "objectID": "mini_project_first.html",
    "href": "mini_project_first.html",
    "title": "Mini Project 1",
    "section": "",
    "text": "I produced four choropleth maps illustrating two different characteristics – one numeric and one categorical – that have been measured for each US state. The numerical data was on obesity data and the categorical data was created from a dataset on the number of bikeshare systems. This data is displayed in both static and interactive forms.\n\n# Loading packages \n\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(maps)\nlibrary(mdsr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Loading and renaming my datasets\n\nlibrary(readr)\nnutrition_physical_data &lt;- read_csv(\"Nutrition__Physical_Activity__and_Obesity_-_Behavioral_Risk_Factor_Surveillance_System (2).csv\")\n\nbikeshare &lt;- read_csv(\"Bikeshare_Scooter_Systems_20240919.csv\") |&gt;\n  rename(state = STATE,\n         city = CITY,\n         citystate = CITYSTATE) |&gt;\n  select(state, city, citystate)\n\n\n# New saved data\nobesity_data &lt;- nutrition_physical_data |&gt;\n  #Renaming variable to make it easier to work with\n  rename(age = `Age(years)`) |&gt;\n  #Filtering for a few relevant variables \n  filter(YearStart == 2020,\n         Question == \"Percent of adults aged 18 years and older who have obesity\", \n         !is.na(Data_Value),\n         age == \"18 - 24\"\n  ) |&gt;\n  #Selecting specific columns\n  select(YearStart,\n         LocationDesc, \n         Question, \n         Data_Value, \n         age,\n         LocationAbbr\n  ) |&gt;\n  #Creating a new column with state names that are lowercase \n  mutate(state = str_to_lower(LocationDesc)) |&gt;\n  select(!LocationDesc) |&gt;\n  #Filter out \n  filter(!state == \"guam\" | !state == \"puerto rico\")\n\n#Bring this dataset in so that it can draw points to create states\nus_states &lt;- map_data(\"state\")\nhead(us_states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\n\nobesity_data |&gt;\n  #Join two datasets with different names for state names (one draws map and the other has numeric variable)\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  rename(region = state) |&gt;\n  #Plotting to create states \n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)\n  ) + \n  #Coloring certain states based on the values in Data_Value\n  geom_polygon(aes(fill = Data_Value), color = \"black\") +\n  scale_fill_viridis(option = \"magma\") +\n  labs(\n    title = \"Percent of Adults Aged 18 years and Older with Obesity,\n    State-Level\",\n    subtitle = \"Data obtained from the U.S. Department of Health & Human Services' \n    Nutrition, Physical Activity, and Obesity - Behavioral Risk Factor Surveillance System\", \n    fill = \"Percentage of Population\",\n    caption = \"Created by Zuzanna Wasiluk for Mini-Project 1\"\n    ) +\n  #Clean white background in the back of the graph\n  coord_map() + \n  theme_void() \n\n\n\n\n\n\n\n\nDescription: This plot describes the percentage of adults who are obese by state as opposed to total population. Generally, most of the states were either magenta or dark purple indicating that they fall between 15-20. Therefore, their percentage of the adult population who have obesity was on the lower half of the scale. There is a concerning pattern of orange and yellow shades in the South region, representing a higher percentage of obese adults in this area than in the rest of the United States. Furthermore, the lightest states are all next to each other which prompts more elaborate investigation into why AR, MS, and AL have such an alarming trend of adult obesity.\nThe link to the website where I found this data can be found here:U.S. Department of Health & Human Services.\nAlt_Text: This is a choropleth plot of the United States (48 states minus Hawaii and Alaska) that visualizes percentages of adult obesity in each state. It has a continuous scale alongside the map graph with a range of 11 to 27, the scale is labeled as “Percentage of Population.” The variables are state and percentage of adult population with obesity. The appearance of the colors for most of the states suggests that the total adult population who have obesity floats around 18 percent (the middle of the scale) for the United States. However, the light yellow and orange color in the Southern region tells us that the percentage of adult obesity is higher in the South.\n\n#Create categorical variable\nbikeshare_state &lt;- bikeshare |&gt;\n  #Group by state to find the number of cities in each state with a bikeshare scooter system\n  group_by(state) |&gt;\n  summarise(Count = n()) |&gt;\n  #Mutate to create a column that takes numeric information and groups it based on which conditions it ultimately meets\n  mutate(Amount = case_when(\n    Count &gt;= 130 ~ \"Very High\",\n    Count &gt;= 100 ~ \"High\",\n    Count &gt;= 80 ~ \"Moderate\",\n    Count &gt;= 40 ~ \"Low Moderate\",\n    Count &gt;= 0 ~ \"Low\"))\n\n\n#Group by to have a dataset to merge with the other one that contains the written out state name instead of abbreviation \nnutrition &lt;- nutrition_physical_data |&gt;\n  group_by(LocationDesc, LocationAbbr) |&gt;\n  summarize(n = n())\n\n\n#Merge the dataset to have bstates with the lowercase versions of the state names \nbstates &lt;- bikeshare_state |&gt;\n  right_join(nutrition, by = c(\"state\" = \"LocationAbbr\")) |&gt;\n  mutate(region = str_to_lower(LocationDesc)) |&gt;\n  select(!n)\n\n\n#Join with the dataset that contains information for drawing the states in the plot using the geometry column\nbstates |&gt;\n  right_join(us_states, by = \"region\")|&gt;\n  #Relevel variables so that they appear in the appropriate order on the plot\n  mutate(Amount = fct_relevel(Amount, \"Very High\", \"High\", \"Moderate\", \"Low Moderate\", \"Low\")) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  #Shade the state shapes by their value in the Amount variable \n  geom_polygon(aes(fill = Amount), color = \"black\", linewidth = 0.2) +\n  #Clean background\n  coord_map() + \n  theme_void() +  \n  scale_fill_viridis_d(option = \"magma\") + \n  labs(\n    title = \"Classification of Bikeshare Scooter Systems in Each \n    State in the United States\",\n    subtitle = \"Data obtained from the U.S. Department of Transportation\n    from their Bureau of Transportation Statistics (BTS)\",\n    fill = \"Classification\",\n    caption = \"Created by Zuzanna Wasiluk for Mini-Project 1\"\n  )\n\n\n\n\n\n\n\n\nThe plot above describes the “classification” of bikeshare scooter systems in US states. To provide context for those less familiar with bikeshare scooter systems, they are a category of transportation that includes small vehicles that are typically found in cities. We find that most US states have a low number of bikeshare scooter systems except states with multiple major US cities (Florida, California, Texas). From the plot before, we see that the states with the highest obesity percentage also have the lowest classification of bikeshare scooter systems. However, this lack does not provide an explanation for the obesity rates because many US states with lower obesity rates also have a low number of bikeshare scooter systems.\nThe data can be found under this link Bureau of Transportation Statistics’ website.\n\nlibrary(leaflet)\nlibrary(sf)\nlibrary(htmltools)\nlibrary(glue)\n\n\n# Create density bins \nweight_int_state &lt;- obesity_data |&gt;\n  mutate(density = cut(Data_Value, n = 6,\n          breaks = c(0, 5, 10, 15, 20, 25, 30))) |&gt;\n  filter(!(state %in% c(\"alaska\", \"hawaii\", \"puerto rico\", \"national\")))\n\n#Create labels by combining multiple strings into one string \nweight_int_state &lt;- weight_int_state |&gt;\n  mutate(labels = str_c(state, \": \", Data_Value, \" % of adult population\"))\n\nlabels &lt;- lapply(weight_int_state$labels, HTML)\n\n#Create a new saved dataset\nleaflet_data &lt;- weight_int_state |&gt;\n  select(density, state, labels, Data_Value)\n\n\nstates &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")  \nclass(states) \n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n#Create a new variable with lowercase letters\nn_states &lt;- states |&gt;\n  mutate(lowname = str_to_lower(name))\n\nnb_states &lt;- n_states |&gt;\n  left_join(leaflet_data, by = c(\"lowname\" = \"state\"))\n#Define bins and palette for leaflet operations\nbins &lt;- c(0, 5, 10, 15, 20, 25, 30)\npal &lt;- colorBin(\"YlOrRd\", domain = nb_states$Data_Value, bins = bins)\n\n\n#Leaflet plot\nleaflet(nb_states) %&gt;%\n  setView(-96, 37.8, 4) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(Data_Value),\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) %&gt;%\n  addLegend(pal = pal, values = ~Data_Value, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\nWarning in sf::st_is_longlat(x): bounding box has potentially an invalid value\nrange for longlat data\n\n\n\n\n\n\nCategorical interactive plot\n\ncat_states &lt;- nb_states |&gt;\n  left_join(bstates, by =c(\"lowname\" = \"region\")) |&gt;\n  #Putting multiple strings in one string using str_c\nmutate(labels = str_c(state, \": \", Amount, \" number of bikeshare systems\")) |&gt;\n  mutate(Amount = as.factor(Amount))\n\nlabels &lt;- lapply(cat_states$labels, HTML)\n#Define levels and factor palette\nlevels(cat_states$Amount)\n\n[1] \"High\"         \"Low\"          \"Low Moderate\" \"Moderate\"     \"Very High\"   \n\nfactpal &lt;- colorFactor(\"viridis\",\n                       levels(cat_states$Amount))\n#Leaflet plot\nleaflet(cat_states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    weight = 2,\n    opacity = 1,\n    color = \"black\",\n    fillColor = ~ factpal(cat_states$Amount),\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = factpal, values = ~cat_states$Amount, \n            opacity = 0.7, title = NULL, position = \"bottomright\")\n\nWarning in sf::st_is_longlat(x): bounding box has potentially an invalid value\nrange for longlat data"
  },
  {
    "objectID": "econ.html",
    "href": "econ.html",
    "title": "Economics",
    "section": "",
    "text": "Submitted: May 21, 2024 Instructor: Professor Colin Harris\nTitle: The Cultural Impact of Joining the European Union on 2004 and 2007 Expansion Countries\nAbstract:\nTwo of the most historically significant expansions in the European Union (EU) occurred in 2004 and 2007 with mostly former post-communist countries and transition economies. These EU enlargements unified the West and East after a period of Eastern isolation from the rest of Europe. Since then, the EU has expanded its influence outside of the purely economic into the social, cultural, and political. With this shift and the goal to curate a national identity for Europe, do member states experience converge to the values of the EU founding members? Using a staggered difference-in-difference model, I compared the average value of 2004/2007 expansion countries to the EU founding member average value before and after treatment (membership into the EU). This study concludes that convergence only occurs for tolerance toward racial minorities and immigrants.\nJEL codes: D91, A13, Z13\nKeywords: social convergence, social divergence, the European Union, 2004 member states, 2007 member states, EU founding members"
  },
  {
    "objectID": "econ.html#download-the-pdf",
    "href": "econ.html#download-the-pdf",
    "title": "Economics",
    "section": "Download the PDF",
    "text": "Download the PDF"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "mini_project_cinderella.html",
    "href": "mini_project_cinderella.html",
    "title": "Mini Project 4",
    "section": "",
    "text": "There are many adaptations of Cinderella’s story in European folklore but we are most familiar with the French version published in 1697 by Charles Perrault. That version was the foundational piece of literature behind the Disney adaptation of Cinderella. Cinderella’s popularity has endured the test of time and is still revered as a classic tale for children around the world. To be accessible to audiences around the world, publishers hire translators to translate the original source material to other languages using the appropriate syntax. This explains why translations differ from language to language and even occasionally, from translator to translator. This project uses text analysis for the Cinderella tale in three languages: French, English and Polish to examine whether there are distinguishable differences between the stories.\nFolklore written by Michalopoulos and Xue (2021) concluded that (1) societies with tales portraying men as dominant and women as submissive relgate women to subordinate positions, (2) more risk-averse and less entrepreneurial people grew up listening to stories wherein competitions and challenges are more likely to be harmful, and (3) communities with low tolerance toward antisocial behavior are more trusting. This paper exmaines unique folklore in cultures to see what effect they have on their population but it may be interesting to study how translations of one story shape interpretations of its moral lessons across cultures."
  },
  {
    "objectID": "mini_project_cinderella.html#understanding-cinderellas-origins",
    "href": "mini_project_cinderella.html#understanding-cinderellas-origins",
    "title": "Mini Project 4",
    "section": "",
    "text": "There are many adaptations of Cinderella’s story in European folklore but we are most familiar with the French version published in 1697 by Charles Perrault. That version was the foundational piece of literature behind the Disney adaptation of Cinderella. Cinderella’s popularity has endured the test of time and is still revered as a classic tale for children around the world. To be accessible to audiences around the world, publishers hire translators to translate the original source material to other languages using the appropriate syntax. This explains why translations differ from language to language and even occasionally, from translator to translator. This project uses text analysis for the Cinderella tale in three languages: French, English and Polish to examine whether there are distinguishable differences between the stories.\nFolklore written by Michalopoulos and Xue (2021) concluded that (1) societies with tales portraying men as dominant and women as submissive relgate women to subordinate positions, (2) more risk-averse and less entrepreneurial people grew up listening to stories wherein competitions and challenges are more likely to be harmful, and (3) communities with low tolerance toward antisocial behavior are more trusting. This paper exmaines unique folklore in cultures to see what effect they have on their population but it may be interesting to study how translations of one story shape interpretations of its moral lessons across cultures."
  },
  {
    "objectID": "mini_project_cinderella.html#vowel-analysis",
    "href": "mini_project_cinderella.html#vowel-analysis",
    "title": "Mini Project 4",
    "section": "Vowel Analysis",
    "text": "Vowel Analysis\nVowels are sounds that can be made with an open vocal tract which contrasts with consonants which have a closure along the vocal tract. During the Romantic period, sound patterns played a key role in literature, especially poetry. By analyzing the number of vowels in each sentence by book to get the proportion of vowels, we see slight differences in how the sentences sound because they have varying levels of vowels. Sentences with a higher proportion of vowels will be expressed differently sonically.\n\n# Finding the number and proportion of vowels in a sentence\n#   for each book\neng_vowel &lt;- english_cinderella |&gt; \n  mutate(num_vowels = str_count(word, \"[aeiouy]\"),\n         prop_vowels = num_vowels/(str_count(word)))\n\nfrench_vowel &lt;- french_cinderella |&gt; \n  mutate(num_vowels = str_count(word, \"[aeiouyéèêëàùç]\"),\n         prop_vowels = num_vowels/(str_count(word)))\n\npolish_vowel &lt;- polish_cinderella |&gt; \n  mutate(num_vowels = str_count(word, \"[aeiouyąęóśćźł]\"),\n         prop_vowels = num_vowels/(str_count(word)))\n\nall_vowels &lt;- rbind(eng_vowel, french_vowel, polish_vowel)\n\n\nlibrary(wesanderson)\n\nWarning: package 'wesanderson' was built under R version 4.4.2\n\nall_vowels |&gt; \n  group_by(title) |&gt; \n  ggplot(aes(x = num_vowels, y = prop_vowels, color = title)) +\n  geom_point(size = 2, alpha = 0.75) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_manual(values = wes_palette(\"Darjeeling1\")) +\n  theme_clean() +\n  labs(x = \"# of vowels per sentence\",\n       y = \"Proportion of vowels in a sentence\",\n       title = \"Vowel Analysis by Cinderella Text\",\n       subtitle = \"Total number of vowels per sentence vs. proportion of vowels in sentence\", \n       color = \"Cinderella Text\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis visualization has the number of vowels per sentence on the x-axis and the proportion of vowels on the y-axis, color coded by the text language. The English language has the lowest proportion of vowels in a sentence compared to French and Polish. The Polish language has the highest proportion of vowels yet it has the same range for the number of vowels in a sentence. The vowel analysis for the French language is interesting because it has the greatest range in number of vowels per sentence which could explain some of its outliers for the proportion of vowels in a sentence."
  },
  {
    "objectID": "mini_project_cinderella.html#most-popular-words-in-cinderella",
    "href": "mini_project_cinderella.html#most-popular-words-in-cinderella",
    "title": "Mini Project 4",
    "section": "Most Popular Words in Cinderella",
    "text": "Most Popular Words in Cinderella\nThese plots visualize the highest frequency words in each of the texts. The bar plots are a static visualization for the words with the highest frequency while the wordcloud is interactive and gives a precise count of the appearance when you hover over the word.\n\ntidy_polish &lt;- polish_cinderella |&gt; \n  mutate(line = row_number()) |&gt;\n  unnest_tokens(word, word, token = \"words\") |&gt; \n  mutate(word2 = fct_collapse(word, \"kopciuszek\" = c(\"rózia\", \"kopciuszek\", \"kopciuszku\"))) |&gt; \n  anti_join(polish_stopwords) |&gt; \n  count(word2, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\nfrench_tidy &lt;- french_cinderella |&gt; \n  mutate(line = row_number()) |&gt;\n  unnest_tokens(word, word, token = \"words\") |&gt; \n  anti_join(french_stopwords) |&gt; \n  count(word, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\ntidy_english &lt;- english_cinderella |&gt; \n  mutate(line = row_number()) |&gt;\n  unnest_tokens(word, word, token = \"words\") |&gt; \n  anti_join(english_stopwords) |&gt; \n  count(word, sort = TRUE) \n\nJoining with `by = join_by(word)`\n\n\n\n# Creating plots with a count of the number of times the most popular words are \n#   used in the text\n\npolish_cinderella |&gt; \n# Creating a new line for rownumber \n  mutate(line = row_number()) |&gt;\n# Seperating sentences into words for word analysis\n  unnest_tokens(word, word, token = \"words\") |&gt; \n  anti_join(polish_stopwords) |&gt; \n# Count of the number of times a word appears\n  count(word, sort = TRUE) |&gt; \n# Top 20 words\n  slice_max(n, n = 20) |&gt; \n  ggplot(aes(fct_reorder(word, n), n)) +\n  geom_col(fill = \"dodgerblue\") +\n  coord_flip() +\n  theme_clean() +\n  labs(x = \"Words in Polish Translation\",\n       y = \"Count of Appearance in Text\", \n       title = \"Most Popular Words in Polish Translation of Cinderella\",\n       subtitle = \"Without the fct_collapse for Cinderella-adjacent terms\")\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\n\n\ntidy_polish |&gt;\n  slice_max(n, n = 20) |&gt;\n  ggplot(aes(fct_reorder(word2, n), n)) +\n  geom_col(fill = \"plum3\") +\n  coord_flip() +\n  theme_clean() +\n  labs(x = \"Words in Polish Translation\",\n       y = \"Count of Appearance in Text\", \n       title = \"Most Popular Words in Polish Translation of Cinderella\")\n\n\n\n\n\n\n\nfrench_cinderella |&gt; \n  mutate(line = row_number()) |&gt;\n  unnest_tokens(word, word, token = \"words\") |&gt; \n  anti_join(french_stopwords) |&gt; \n  count(word, sort = TRUE) |&gt; \n  slice_max(n, n = 20) |&gt;\n  ggplot(aes(fct_reorder(word, n), n)) +\n  geom_col(fill = \"lightpink\") +\n  coord_flip() +\n  theme_clean() +\n  labs(x = \"Words in Original French\",\n       y = \"Count of Appearance in Text\",\n       title = \"Most Popular Words in Original French Cinderella\")\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\n\n\ntidy_english |&gt; \n  slice_max(n, n = 20) |&gt;\n  ggplot(aes(fct_reorder(word, n), n)) +\n  geom_col(fill = \"lightblue\") +\n  coord_flip() +\n  theme_clean() +\n  labs(x = \"Words in English Translation\",\n       y = \"Count of Appearance in Text\",\n       title = \"Most Popular Words in English Translation of Cinderella\")\n\n\n\n\n\n\n\n\nWe see that once we combine appearances of “kopciuszek” and “rozia” together in the Polish translation, the most popular word among all of texts is Cinderella. The Polish translation is interesting in that Cinderella (Kopciuszek) is a mean-spirited nickname given to her while Rozia is her birth name. It is interesting that we find out Cinderella’s birth name in the Polish translation of the tale and see the narrator refer to her by her birth name. The kopciuszek/rozia observation was eye-catching because when we include word rozia, the Polish Cinderella is the only translation that does not have “Cinderella” as the most common word despite both “kopciuszek” and “rozia” being used as her name. There are certainly arguments in support and against highlighting the appearance of rozia in the text. Otherwise, the translations share a lot of the same words such as “godmother,” “sisters,” “ball,” “kitchen,” “ash/cinder,” “king,” and “slipper.” However, the frequency in which the words appear is not the same across translations. “Beautiful” is a highly-used word in all of the translations but the amount of times it appears in the French text is much higher than the others. A linguist could make valuable observations based on which words/themes appear more commonly in some translations of classic childrens stories than others.\n\n#Wordcloud for original French text\nwords_french &lt;- french_tidy |&gt;\n  slice_head(n = 200) |&gt;\n  data.frame()\n\nwordcloud2(\n  words_french, \n  size = .25, \n  shape = 'circle'\n)"
  },
  {
    "objectID": "mini_project_cinderella.html#sentiment-analysis",
    "href": "mini_project_cinderella.html#sentiment-analysis",
    "title": "Mini Project 4",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nSentiment analysis takes words and sorts them as either having a “positive” or “negative” sentiment. The English sentiment lexicons are built out in R with different options (bing, nrc, afinn) to analyze words in the text. The French lexicon was found through a GitHub repository which resembled the bing classification in the get_sentiments function. In this analysis, we compare the sentiments for the French and English versions of Cinderella because the Polish lexicon is not nearly as built out.\n\n# Read in French sentiment lexicons from GitHub\nlexicon_fr_neg &lt;- readLines(\"negative_words_fr.txt\") |&gt; \n  as_tibble(word = lexicon_fr_neg) |&gt;\n  mutate(sentiment = \"negative\")\nlexicon_fr_pos &lt;- readLines(\"positive_words_fr.txt\") |&gt; \n  as_tibble(word = lexicon_fr_pos) |&gt;\n  mutate(sentiment = \"positive\")\n\nfrench_lexicon &lt;- rbind(lexicon_fr_pos, lexicon_fr_neg)\n\n# Using the french lexicon\nfrench_tidy |&gt; \n# Join with lexicon using common variable\n  inner_join(french_lexicon, by = c(\"word\" = \"value\")) |&gt;\n# Creating a group of highest sentiments\n  group_by(sentiment) |&gt;\n  slice_max(n, n = 5) |&gt;\n  ungroup() |&gt;\n# Making a plot for sentiment analysis\n  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +\n    geom_col() + \n    scale_fill_manual(values = wes_palette(\"Moonrise3\")) +\n    coord_flip() +\n    theme_clean() +\n    labs(y = \"Count of appearance in text\",\n         x = \"Words from original French text\",\n         fill = \"Sentiment\",\n         title = \"Sentiment Analysis for Original French\")\n\n\n\n\n\n\n\nenglish_lexicon &lt;- get_sentiments(lexicon = \"bing\")\n\ntidy_english |&gt; \n  inner_join(english_lexicon) |&gt; \n  group_by(sentiment) |&gt;\n  slice_max(n, n = 5) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +\n    geom_col() + \n    scale_fill_manual(values = wes_palette(\"Zissou1\")) +\n    coord_flip() +\n    theme_clean() +\n    labs(y = \"Count of appearance in text\",\n         x = \"Words from English translation\",\n         fill = \"Sentiment\",\n         title = \"Sentiment Analysis for English Translation\")\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\n\n\n\nThe English version of the sentiment analysis shows that there is a higher number of negative sentiment words but the positive sentiment words have a higher frequency. On the other hand, the French version has a higher number of positive sentiment words and a higher frequency. Therefore, we can conclude that there is generally a more positive sentiment in the French version of Cinderella as opposed to the English one."
  },
  {
    "objectID": "mini_project_cinderella.html#progression-of-story",
    "href": "mini_project_cinderella.html#progression-of-story",
    "title": "Mini Project 4",
    "section": "Progression of Story",
    "text": "Progression of Story\nStories can feel fast-paced or not depending on the number of words chosen and the placement of lengthier sentences. Word choice matters but the length in which it is all read plays a significant role as well. I wanted to look at the lengthiest parts of the book (the chapters with the highest number of words) and see where they were located.\n\n# Creating  a dataframe with a variable that does not have -- and setting up an index\n#   for a chapter to have 9 lines\npolish_cinderella &lt;- tibble(word = cinderella_pl) |&gt; \n  mutate(title = \"Polish\",\n         word2 = str_replace_all(word,\"--\", \"\"),\n         linenumber = row_number(),\n         index = linenumber %/% 9) |&gt; \n  select(word2, linenumber, index, title) \n\nenglish_cinderella &lt;- english_cinderella |&gt; \n  mutate(word2 = str_replace_all(word,\"--\", \"\"),\n         linenumber = row_number(),\n         index = linenumber %/% 9,\n         title = \"English\") |&gt; \n  select(word2, linenumber, index, title)\n  \nfrench_cinderella &lt;- french_cinderella |&gt; \n  mutate(word2 = str_replace_all(word,\"--\", \"\"),\n         linenumber = row_number(),\n         index = linenumber %/% 9,\n         title = \"French\") |&gt; \n  select(word2, linenumber, index, title)\n\ncinderella &lt;- rbind(polish_cinderella, french_cinderella, english_cinderella)\n\n# Using the data frame with all of the other data frames\ncinderella |&gt; \n# Grouping indexes by titles\n  group_by(title, index) |&gt; \n# Finding the number of words using str_count\n  mutate(word_count = str_count(word2, \"\\\\b[^ ]+\\\\b\")) |&gt; \n# Count of total words in the index\n  summarize(index1 = sum(word_count)) |&gt; \n# Creating a plot that is faceted by title\nggplot(aes(x = index, y = index1, fill = title)) + \n  geom_col() +\n  facet_wrap(~title) +\n  scale_fill_manual(values = wes_palette(\"Zissou1\")) +\n  theme_clean() +\n  labs(title = \"Word Count by Chapter in Each Book\",\n       x = \"Chapter\",\n       y = \"Count of words in chapter\",\n       color = \"Language of book\")\n\n`summarise()` has grouped output by 'title'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nWe hope to understand the number of words in each chapter to compare the books by their length and the points during the book (beginning, middle, and end) in which they are the most “lengthy.” The English translation has a U-shape that shows the beginning and end as being the most “wordy.” The French shows the opposite, with the bars in the middle supporting the claim that the middle of the French Cinderella story is longer than other parts in the book. Surprisingly, the Polish version of Cinderella is fairly consistent in the number of words for its chapters from beginning to end. We can gather that the different parts of each book have the most words/feel wordy and it is not uniform across all languages."
  },
  {
    "objectID": "mini_project_cinderella.html#punctuation-marks-in-cinderella",
    "href": "mini_project_cinderella.html#punctuation-marks-in-cinderella",
    "title": "Mini Project 4",
    "section": "Punctuation Marks in Cinderella",
    "text": "Punctuation Marks in Cinderella\nPunctuation marks are important to storytelling. The number of exclamation marks and question marks dictates the tone of the story. By examining the occurrence of these punctuation marks, we can see moments of excitement or confusion reflected in the story.\n\n# Creating a new data frame called punct_marks to include excitement and confusion\n#   the measure for this was whether the sentence began or ended with a ! or ?\npunct_marks &lt;- cinderella |&gt; \n  mutate(excite = str_detect(word2, \"!$\"),\n         confuse = str_detect(word2, \"\\\\?$\")) \n\npunct_marks |&gt; \n  group_by(title, excite) |&gt; \n  count(excite) |&gt; \n  filter(!is.na(excite)) |&gt; \n  ungroup() |&gt; \n  group_by(title) |&gt; \n# Creating new variables to find the total amount of TRUE and FALSE values for \n#   presence of exclamation points\n  mutate(total = sum(n),\n         prop = n/total) |&gt; \n# Making a ploy with excitement on the x-axis, proportion on the y, with title as a fill\n  ggplot(aes(x = excite, y = prop, fill = excite)) +\n  geom_col() +\n  facet_wrap(~title) +\n  theme_clean() +\n  scale_fill_manual(values = wes_palette(\"FrenchDispatch\")) +\n  labs(title = \"Exclamation marks in Cinderella Texts\",\n       subtitle = \"TRUE is sentences with ! and FALSE is sentences without !\",\n       x = \"Exclamation marks\",\n       y = \"Proportion of sentences\",\n       fill = \"Excitement in sentence\")\n\n\n\n\n\n\n\n\nThis graph shows the proportion of exclamation marks in sentences by book to reveal that the French have a lower proportion of sentences with exclamation points than Polish and English. The English and Polish translations are pretty evenly matched with the proportion of sentences with exclamation points.\n\npunct_marks |&gt; \n# Taking out NAs\n  filter(!is.na(excite)) |&gt; \n# Linenumber on the x with the title on the y (language text), colored by the presence\n#   of exclamation marks\n  ggplot(aes(x = linenumber, y = title, color = excite)) +\n  geom_point(size = 2, alpha = 0.5) +\n  theme_clean() +\n  scale_color_manual(values = wes_palette(\"FrenchDispatch\")) +\n  labs(y = \"Language of Cinderella Text\",\n       title = \"Tracking Where ! Occurs in Each Book\",\n       subtitle = \"Each point is a line in the book\",\n       x = \"Linenumber\",\n       color = \"Excitement in sentence\")\n\n\n\n\n\n\n\n\nThere is a delicate balance writers/translators need to maintain in working with exclamation marks. If you use too many, the excitement is watered down. Using them sparingly might stick out more to readers than writers may intend them to. With this in mind, I wanted to analyze how often a sentence appears with an exclamation mark by having each point represent a sentence. The blue/green points represent the sentences without exclamation marks and the red points are sentences with an exclamation mark. Visually, we see that the French version has very few red points which indicates that exclamation points are not used frequently in this Cinderella story. Sentences that have exclamation marks in the English translation tend to occur near each other while the Polish translation has nearly equidistant spaces in between each other. The key takeaway is that the translations of the original French use exclamation points more than the original text.\n\n# This code is nearly identical to the exclamation mark code except it is for\n#   question marks in each line for the books\npunct_marks |&gt; \n  group_by(title, confuse) |&gt; \n  count(confuse) |&gt; \n  filter(!is.na(confuse)) |&gt; \n  ungroup() |&gt; \n  group_by(title) |&gt; \n  mutate(total = sum(n),\n         prop = n/total) |&gt; \n  ggplot(aes(x = confuse, y = prop, fill = confuse)) +\n  geom_col() +\n  facet_wrap(~title) +\n  theme_clean() +\n  scale_fill_manual(values = wes_palette(\"FrenchDispatch\")) +\n  labs(title = \"Question marks in Cinderella Texts\",\n       subtitle = \"TRUE is sentences with ? and FALSE is sentences without ?\",\n       x = \"Question marks\",\n       y = \"Proportion of sentences\",\n       fill = \"Confusion in sentences\")\n\n\n\n\n\n\n\n\nThis graph shows the proportion of question marks in sentences by book to reveal that the French have a higher proportion of sentences with question marks than Polish and English. The English translation has a very low proportion of sentences with question marks compared to the proportion of sentences with exclamation points. Apart from the French text, there are lower proportions of question marks in the sentences than exclamation points.\n\n# This line is also nearly identical to that of the exclamation mark code\npunct_marks |&gt; \n  filter(!is.na(confuse)) |&gt; \n  ggplot(aes(x = linenumber, y = title, color = confuse)) +\n  geom_point(size = 2, alpha = 0.5) +\n  theme_clean() +\n  scale_color_manual(values = wes_palette(\"FrenchDispatch\")) +\n  labs(y = \"Language of Cinderella Text\",\n       title = \"Tracking Where ? Occurs in Each Book\",\n       subtitle = \"Each point is a line in the book\",\n       x = \"Linenumber\",\n       color = \"Confusion in sentence\")\n\n\n\n\n\n\n\n\nQuestion marks are useful indicators of tone. In reading aloud a story, it is pretty clear that you will need to change your voice to be more inquisitive when you hit a question mark. Therefore, having many question marks can affect the tone and progression of the story. Our plot aims to see the presence of question marks over the entire book. The French uses more question marks than any other version of the Cinderella story even though they used the least amount of exclamation marks. Are the French more curious than excited? The English translation shows the translator used question marks sparingly and the Polish translation uses a few as well. From this, we can gather that translators did not adopt the trend of question marks in the original French version because the English and Polish versions use less in proportion to the rest of their book."
  },
  {
    "objectID": "mini_project_cinderella.html#conclusion",
    "href": "mini_project_cinderella.html#conclusion",
    "title": "Mini Project 4",
    "section": "Conclusion",
    "text": "Conclusion\nI hope you enjoyed reading this report as much as I enjoyed making it. I gained some valuable insight into the art of translating a classic story for masses with different cultures and language patterns. The trend that I saw in the original French text was not what I always saw in the translations.\nTo summarize the study, I explore the number and proportion of vowels, conduct a count of popular words (+ wordcloud), perform sentiment analysis, envision the number of words per chapter, and visualize the use of punctuation throughout the texts. All of this was done to find out the degree to which translations are different from the original source material and whether the differences in these could affect how it is read/interpreted by audiences."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zuzanna Wasiluk",
    "section": "",
    "text": "Education\nBachelor of Arts in Economics Statistics and Data Science concentration Expected Graduation Date May 2025 | Saint Olaf College\n\n\nCurrent Courses\nEconometrics: Time Series and Forecasting Data Science 2 Statistics 2 Politics and Development\n\n\nMembership Organizations\nGirls Write Now Rewriting the Code Women in Economics at St. Olaf College International Association of Feminist Economics"
  }
]